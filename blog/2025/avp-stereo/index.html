<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Stereo Matching and Depth Map Creation on the Vision Pro | Griffin J. Hurt </title> <meta name="author" content="Griffin J. Hurt"> <meta name="description" content="Implementing stereo matching on AVP with RAFT-Stereo"> <meta name="keywords" content="researcher, scientist, mixed-reality, spatial-computing, hci, human-computer-interaction"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://griffinhurt.com/blog/2025/avp-stereo/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Griffin</span> J. Hurt </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Stereo Matching and Depth Map Creation on the Vision Pro</h1> <p class="post-meta"> June 13, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/research"> <i class="fa-solid fa-hashtag fa-sm"></i> research</a>   <a href="/blog/tag/vision-pro"> <i class="fa-solid fa-hashtag fa-sm"></i> vision-pro</a>   <a href="/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> computer-vision</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p><em>TLDR: I implemented Stereo Matching on the Vision Pro using RAFT-Stereo and visionOS 26! <a href="https://github.com/surreality-lab/VisionProDisparityMatching" rel="external nofollow noopener" target="_blank">Source code is here!</a></em></p> <p>With the recent release of visionOS 26 at WWDC 2025, Apple has finally <a href="https://developer.apple.com/videos/play/wwdc2025/223/" rel="external nofollow noopener" target="_blank">given developers access to the right camera on the Vision Pro</a> (<em>at least, developers with an enterprise license</em>). This greatly expands the capability of the Vision Pro for environment understanding and registration by enabling depth to be calculated from a rectified stereo pair (see <a href="https://en.wikipedia.org/wiki/Binocular_vision" rel="external nofollow noopener" target="_blank">binocular vision</a> and <a href="https://en.wikipedia.org/wiki/Epipolar_geometry" rel="external nofollow noopener" target="_blank">epipolar geometry</a>). These last few days, I’ve been trying to put together a reasonable pipeline for stereo matching and, ergo, depth map calculation on the Vision Pro. While calculating depth still eludes me, I at least have a reasonable pipeline for disparity estimation on the Vision Pro.</p> <p><em>(Note: I’m going to use the Swift version of <code class="language-plaintext highlighter-rouge">ARKit</code> throughout this article so that the syntax is a bit easier to follow. There’s also a <a href="https://developer.apple.com/documentation/arkit/arkit-in-visionos-c-api" rel="external nofollow noopener" target="_blank">C API</a> that might be more useful for developers building in support to existing libraries.)</em></p> <h2 id="getting-frames-from-the-main-camera-on-the-apple-vision-pro">Getting Frames from the Main Camera on the Apple Vision Pro</h2> <p>The first step to doing any type of depth or disparity calculation on the Vision Pro is to get camera frames from both the left and right cameras using the <code class="language-plaintext highlighter-rouge">ARKit</code> API. The <a href="https://developer.apple.com/documentation/visionos/accessing-the-main-camera" rel="external nofollow noopener" target="_blank">“Accessing The Main Camera” Xcode project</a> from Apple provides a good codebase to start from, but I’ll briefly explain the process here as well.</p> <p>In <code class="language-plaintext highlighter-rouge">ARKit</code>, you can instantiate a session (<a href="https://developer.apple.com/documentation/arkit/arkitsession" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">ARKitSession</code></a>) that consists of multiple providers (<a href="https://developer.apple.com/documentation/arkit/dataprovider" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">DataProvider</code></a>), each delivering some type of AR data to the application. A basic example would be a <a href="https://developer.apple.com/documentation/arkit/worldtrackingprovider" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">WorldTrackingProvider</code></a> that provides information about the device pose and “anchors” in the user’s surroundings (I would imagine this is what Unity is using under the hood to position the XR camera). When you want to access camera data, you instantiate a <a href="https://developer.apple.com/documentation/arkit/cameraframeprovider" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">CameraFrameProvider</code></a> and add it to your <code class="language-plaintext highlighter-rouge">ARKitSession</code> in <code class="language-plaintext highlighter-rouge">session.run([cameraFrameProvider, &lt;whatever other providers you need&gt;])</code>. After you start the session with your frame provider (ensuring that the frame provider is supported with <code class="language-plaintext highlighter-rouge">CameraFrameProvider.isSupported</code>), you listen to frames by creating an async for loop over <a href="https://developer.apple.com/documentation/arkit/cameraframeprovider/cameraframeupdates(for:)" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">cameraFrameProvider.cameraFrameUpdates(for: someFormat)</code></a>, where <code class="language-plaintext highlighter-rouge">someFormat</code> is a format picked from <code class="language-plaintext highlighter-rouge">CameraVideoFormat.supportedVideoFormats</code>. In visionOS 26, you have access to <a href="https://developer.apple.com/documentation/arkit/cameraframeprovider/camerarectification" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">CameraFrameProvider.CameraRectification</code></a>, so make sure you pick a video format that’s stereo rectified. This eliminates the need for a library like OpenCV to <a href="https://en.wikipedia.org/wiki/Image_rectification" rel="external nofollow noopener" target="_blank">stereo rectify</a> the left and right image feeds (in technical terms, aligning the <a href="https://en.wikipedia.org/wiki/Epipolar_geometry#Epipolar_line" rel="external nofollow noopener" target="_blank">epipolar lines</a> to be horizontal).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pixel_formats-480.webp 480w,/assets/img/pixel_formats-800.webp 800w,/assets/img/pixel_formats-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/pixel_formats.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><small>Figre 1: Some examples of pixel formats available on the Vision Pro. The 864 x 704 pixel, <code class="language-plaintext highlighter-rouge">stereoCorrected</code> format is probably the most useful for this tutorial. Pixel formats are in <a href="https://en.wikipedia.org/wiki/FourCC" rel="external nofollow noopener" target="_blank">FourCC</a> format, so when you see “875704422”, it’s just the int32 representation of “420f” (YUV 4:2:0 chroma subsampling or <a href="https://developer.apple.com/documentation/corevideo/kcvpixelformattype_420ypcbcr8biplanarfullrange" rel="external nofollow noopener" target="_blank">kCVPixelFormatType_420YpCbCr8BiPlanarFullRange</a>).</small></p> <p>Once you have your main loop, you can access “samples” (a bundle containing the image, camera intrinsics, and camera extrinsics) from the left and right cameras using <code class="language-plaintext highlighter-rouge">cameraFrame.sample(for: .left)</code> and <code class="language-plaintext highlighter-rouge">cameraFrame.sample(for: .right)</code> (the latter is new in visionOS 26). You can access the image from a sample as a <code class="language-plaintext highlighter-rouge">CVPixelBuffer</code> through <code class="language-plaintext highlighter-rouge">sample.pixelBuffer</code> (Xcode 26 will yell at you for this, since <a href="https://developer.apple.com/documentation/CoreVideo/cvpixelbuffer-q2e" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">CVPixelBuffer</code></a> is deprecated and being replaced with <a href="https://developer.apple.com/documentation/corevideo/cvbuffer-nfm" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">CVBuffer</code></a> despite its lack of support in other frameworks… <em>foreshadowing</em>). Now that we have the images, we have to figure out a way to calculate disparity per pixel so that we can calculate depth.</p> <h2 id="calculating-disparity-from-a-stereo-pair">Calculating Disparity from a Stereo Pair</h2> <p><a href="https://en.wikipedia.org/wiki/Binocular_disparity" rel="external nofollow noopener" target="_blank">Disparity</a> refers to the distance a pixel travels along the x axis between the left and right camera in a stereo pair. In a stereo rectified image, (once again, an image in which epipolar lines are all horizontal), disparity is also the distance a pixel travels along its epipolar line, which has a lot of importance for calculating depth. The relationship between depth and disparity is inversely proportional (assuming the same <a href="https://en.wikipedia.org/wiki/Pinhole_camera_principal_point" rel="external nofollow noopener" target="_blank">principal point</a> in both cameras):</p> \[\text{depth} = \frac{\text{focal length} \times \text{baseline}}{\text{disparity}}\] <p>I spend a lot of time thinking about ways to convert stereo pairs of images to disparity maps, as it’s an important first step in photogrammetry and stereo reconstruction. There are a multitude of options for this problem including classical approaches (<a href="https://docs.opencv.org/3.4/d9/dba/classcv_1_1StereoBM.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">StereoBM</code></a> and <a href="https://docs.opencv.org/4.x/d2/d85/classcv_1_1StereoSGBM.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">StereoSGBM</code></a> in OpenCV, <a href="https://wildboar-dev.github.io/PatchMatch/" rel="external nofollow noopener" target="_blank">PatchMatch stereo</a>, etc.), ML approaches (the cutting edge <a href="https://nvlabs.github.io/FoundationStereo/" rel="external nofollow noopener" target="_blank">FoundationStereo</a>, <a href="https://research.google/pubs/stereonet-guided-hierarchical-refinement-for-edge-aware-depth-prediction/" rel="external nofollow noopener" target="_blank">StereoNet</a> from Google, <a href="https://arxiv.org/abs/2109.07547" rel="external nofollow noopener" target="_blank">RAFT-Stereo</a>, etc.), and even combined approaches (<a href="https://arxiv.org/abs/2012.01411" rel="external nofollow noopener" target="_blank">PatchMatchNet</a>). <code class="language-plaintext highlighter-rouge">StereoBM</code> is often the first choice of developers due to its integration with OpenCV and fast computation, but I find it leaves a lot to be desired in user experience (you have to spend a lot of time tuning the parameters) and pixel density. I chose to implement <a href="https://github.com/princeton-vl/RAFT-Stereo" rel="external nofollow noopener" target="_blank">RAFT-Stereo</a> for this experiment due to its relatively fast inference time (~45ms on neural cores) and comparatively dense estimation. There’s even a <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Cheng_Stereo_Matching_in_Time_100_FPS_Video_Stereo_Matching_for_WACV_2024_paper.pdf" rel="external nofollow noopener" target="_blank">paper that expands RAFT-Stereo to 100+ FPS</a> by warm-starting the model on previous frames, but there’s no PyTorch implementation, so this remains as an exercise for future work.</p> <h2 id="bundling-raft-stereo-for-vision-pro">Bundling RAFT-Stereo for Vision Pro</h2> <p>In today’s world, there’s so many ways to run a machine learning model on embedded hardware. The option that I like the most (and that I wish had more widespread adoption) is <a href="https://onnx.ai/" rel="external nofollow noopener" target="_blank">ONNX</a> (Open Neural Network eXchange), but interoperability sometimes begets inefficiency, and I realized that my only option was going to be a pure <a href="https://developer.apple.com/documentation/coreml/" rel="external nofollow noopener" target="_blank">CoreML</a> model to leverage the Neural Cores on the Vision Pro. <em>(Yes, ONNX has a <a href="https://onnxruntime.ai/docs/execution-providers/CoreML-ExecutionProvider.html" rel="external nofollow noopener" target="_blank">CoreML backend</a>, but CoreML tools dropped support for ONNX a while back and operation types like <a href="https://onnx.ai/onnx/operators/onnx__GridSample.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">GridSample</code></a> don’t seem to have support for conversion…)</em></p> <p>So, I set out to convert the RAFT-Stereo PyTorch implementation to a CoreML model using <a href="https://apple.github.io/coremltools/docs-guides/" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">coremltools</code></a>. Unfortunately, naïve conversion of the real-time model (specifically using <code class="language-plaintext highlighter-rouge">torch.jit.trace</code>) did not work due to a 7 dimension tensor in the <a href="https://github.com/princeton-vl/RAFT-Stereo/blob/main/core/raft_stereo.py#L55C9-L55C22" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">upsample_flow</code></a> function because CoreML only supports 5D tensors (<code class="language-plaintext highlighter-rouge">torch.export.export</code> didn’t work either because the model was in the <code class="language-plaintext highlighter-rouge">TRAINING</code> dialect even though I put the model in <code class="language-plaintext highlighter-rouge">eval</code> mode…). The 7D tensor gets passed through a softmax layer, so I took great care in ensuring model functionality was preserved when reshaping. After solving that issue (and switching <code class="language-plaintext highlighter-rouge">corr_implementation</code> to <code class="language-plaintext highlighter-rouge">reg</code> instead of <code class="language-plaintext highlighter-rouge">alt</code>), the <code class="language-plaintext highlighter-rouge">torch.jit.trace</code> method for conversion worked like a charm, and I was able to save the model to a <code class="language-plaintext highlighter-rouge">.mlpackage</code> (specifically on 512x512 color images).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/raft_stereo_out-480.webp 480w,/assets/img/raft_stereo_out-800.webp 800w,/assets/img/raft_stereo_out-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/raft_stereo_out.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><small>Figure 2: Example output from my RAFT-Stereo CoreML model run on 512x512 images. The underlying stereo image was taken from a spatial video captured on Vision Pro.</small></p> <p><em>Note: I know <a href="https://github.com/pytorch/executorch" rel="external nofollow noopener" target="_blank">ExecuTorch</a> is designed to simplify situations exactly like this, but for some reason I couldn’t get the Swift package to work with visionOS. The xcframeworks compiled just fine with the <a href="https://github.com/leetal/ios-cmake" rel="external nofollow noopener" target="_blank">iOS CMake toolchain</a>, but the Swift package wouldn’t cooperate. (Also the XNNPACK backend wouldn’t build for some reason…)</em></p> <h2 id="testing-the-raft-stereo-coreml-model">Testing the RAFT-Stereo CoreML Model</h2> <p>After I was able to produce the RAFT-Stereo CoreML model, I wanted to make sure that I was able to run it somewhat efficiently on hardware. Some basic tests with <code class="language-plaintext highlighter-rouge">coremltools</code> in Python (see Figure 2) validated that the output was consistent with what I would expect, so I knew that the model was working properly at a mechanical level at least. Running some performance reports on my Mac indicated that most of the network modules were running on neural cores and that the median inference time was around 45ms, which sounded fantastic to me. Unfortunately, it appears that the neural cores on the Vision Pro are not quite as robust: performance tests on device showed that the majority of modules were running on GPU with a median inference time of around 135ms. I came to learn that this is due to some 32-bit float (FP32) weights sticking around in the model. I tried my best to fix this by turning off the <code class="language-plaintext highlighter-rouge">mixed_precision</code> flag in the model and exporting the CoreML model with <code class="language-plaintext highlighter-rouge">compute_precision=coremltools.precision.FLOAT16</code>, but nothing seemed to fix it. Perhaps quantizing the model with PyTorch before tracing would be more productive. Either way, the model was within a reasonable margin of latency for me, and I wanted to implement a full pipeline with camera capture and the model to get disparity maps on the Vision Pro.</p> <h2 id="putting-it-all-together">Putting it All Together</h2> <p>I started by duplicating the “Accessing the Main Camera” demo project from Apple and adding my <code class="language-plaintext highlighter-rouge">Enterprise.license</code> file. A breaking change in Enterprise API main camera access that doesn’t seem to be documented anywhere is that <code class="language-plaintext highlighter-rouge">NSMainCameraUsageDescription</code> has replaced <code class="language-plaintext highlighter-rouge">NSEnterpriseMCAMUsageDescription</code> in <code class="language-plaintext highlighter-rouge">Info.plist</code>, so make sure you add a usage description under that key. <em>(Also be sure you install the Metal toolchain for visionOS 26 because sometimes it doesn’t install by default and your application fails silently to a runtime error. Ask me how I know!)</em></p> <p>Once the basic application was running (and after checking the <code class="language-plaintext highlighter-rouge">.right</code> camera sample was available), I started modifying the code to rescale and crop the left and right <code class="language-plaintext highlighter-rouge">CVPixelBuffer</code> objects to 512x512 for model inference. (I used my local LLM for help at this point since <code class="language-plaintext highlighter-rouge">CoreVideo</code> and <code class="language-plaintext highlighter-rouge">CoreImage</code> can sometimes trip me up.) After verifying that the buffers were scaled down to 512x512, I tried to import the model by dragging the <code class="language-plaintext highlighter-rouge">.mlpackage</code> into my Xcode project. The default initializer in the automatically generated Swift class for the model caused the runtime error “<code class="language-plaintext highlighter-rouge">error: ANE cannot handle intermediate tensor type fp32</code>”, which I figured was related to the mixed precision issue I saw in my model tests. To get around this, I simply passed a <a href="https://developer.apple.com/documentation/coreml/mlmodelconfiguration" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">MLModelConfiguration</code></a> with <code class="language-plaintext highlighter-rouge">computeUnits</code> set to <code class="language-plaintext highlighter-rouge">.cpuAndGPU</code> in the initialization function and the error went away.</p> <p>After that, all I had to do was implement a function that converted the <a href="https://developer.apple.com/documentation/coreml/mlmultiarray" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">MLMultiArray</code></a> that came out of my CoreML model to a <code class="language-plaintext highlighter-rouge">CVPixelBuffer</code> for presentation, and boom! I was able to get a ~9fps feed of disparity calculation on the Vision Pro!</p> <figure> <video src="/assets/video/Stereo3.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> <p><em>Note: Yes, I know <code class="language-plaintext highlighter-rouge">CVPixelBuffer</code> is deprecated in visionOS 26. Unfortunately, it appears the CoreML API has not been updated to accept <code class="language-plaintext highlighter-rouge">CVBuffer</code> objects yet, so I have to stick with <code class="language-plaintext highlighter-rouge">CVPixelBuffer</code> objects in the meantime.</em></p> <h2 id="the-future-and-mistakes-made">The Future and Mistakes Made</h2> <p>The obvious next step for stereo matching on the AVP is to convert the disparity map to a depth map via the standard equation and then produce a point cloud using the camera intrinsics. I’m working on an implementation that uses <code class="language-plaintext highlighter-rouge">Accelerate.framework</code> to speed up the vector operations and sends the results to Unity after computation, but the extrinsics calculations and interop are still confusing to me.</p> <p>Of course, my implementation is inefficient in some areas. The most obvious area of improvement is in the CoreML model. If I could get it to be entirely FP16, I could utilize the neural cores and hopefully get the inference time back into the ~50ms range; I just haven’t figured it out yet. Additionally, my implementation of <code class="language-plaintext highlighter-rouge">multiArrayToRGBA</code> is almost certainly inefficient given that I’m performing the conversion in a big for loop (the <code class="language-plaintext highlighter-rouge">vDSP</code> methods in <code class="language-plaintext highlighter-rouge">Accelerate.framework</code> may be much faster). I did my best to use <code class="language-plaintext highlighter-rouge">CVPixelBufferPool</code> allocators where possible and reused the <code class="language-plaintext highlighter-rouge">CIContext</code> for rendering, but perhaps someone with more iOS/visionOS development experience could improve the code.</p> <p>Suffice to say, the <a href="https://github.com/surreality-lab/VisionProDisparityMatching" rel="external nofollow noopener" target="_blank">source code is available on GitHub</a> and I welcome any type of improvement to my code or future innovation. Remember to include your <code class="language-plaintext highlighter-rouge">Enterprise.license</code>, and happy developing!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/cutf-mid/">The “Big Ideas” of Teaching</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/cutf-final/">My CUTF: Learning Never Ends</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/blog-title/">What's with the blog's name?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/cutf-intro/">My CUTF Introduction</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Griffin J. Hurt. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 13, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-HCEXTDEN5L"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-HCEXTDEN5L");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>